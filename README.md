# Graph RAG Pipeline - Proof of Concept

A locally-executable Graph RAG (Retrieval-Augmented Generation) pipeline that constructs knowledge graphs from Wikipedia articles and enables LLM-powered querying.

## ğŸ¯ Project Overview

This proof of concept demonstrates:
- **Knowledge Graph Construction**: Automated entity extraction and relationship mapping from Wikipedia articles
- **Multi-Model NER**: Combining GLiNER and spaCy for comprehensive entity recognition
- **Interactive Visualization**: Graph exploration through web-based interfaces
- **Intelligent Querying**: LLM-powered question answering using graph context

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- 8GB RAM minimum
- 10GB free disk space (for models)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/graph-rag-poc.git
cd graph-rag-poc
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
3. Install dependencies:
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

4. Install Ollama and download a model:
```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama service
ollama serve

# In another terminal, pull a model
ollama pull phi3:mini
```

### Basic Usage

```python
# 1. Ingest Wikipedia articles
python -m src.ingestion.wikipedia --topics "Artificial Intelligence,Machine Learning" --max-articles 10

# 2. Extract entities and build graph
python -m src.extraction.pipeline --input data/articles --output data/graphs

# 3. Launch validation interface
streamlit run src/validation/app.py
# 4. Query the graph
python -m src.query.interface --question "What is machine learning?"
```

## ğŸ“ Project Structure

```
graph-rag-poc/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package setup
â”œâ”€â”€ .env.example             # Environment variables template
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingestion/           # Wikipedia data ingestion
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ wikipedia.py     # Wikipedia API wrapper
â”‚   â”‚   â””â”€â”€ preprocessor.py  # Text cleaning
â”‚   â”‚
â”‚   â”œâ”€â”€ extraction/          # Entity extraction pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gliner_extractor.py  # GLiNER model
â”‚   â”‚   â”œâ”€â”€ spacy_extractor.py   # spaCy NER
â”‚   â”‚   â””â”€â”€ pipeline.py          # Combined pipelineâ”‚   â”‚
â”‚   â”œâ”€â”€ graph/               # Graph construction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ builder.py       # NetworkX graph builder
â”‚   â”‚   â”œâ”€â”€ relationships.py # Relationship extraction
â”‚   â”‚   â””â”€â”€ visualizer.py    # PyVis visualization
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/          # Manual validation UI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py          # Streamlit application
â”‚   â”‚   â””â”€â”€ operations.py   # Graph editing operations
â”‚   â”‚
â”‚   â””â”€â”€ query/              # LLM query interface
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ indexer.py      # ChromaDB indexing
â”‚       â”œâ”€â”€ retriever.py    # Context retrieval
â”‚       â””â”€â”€ interface.py    # Query processing
â”‚
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ articles/          # Wikipedia articles (JSON)
â”‚   â”œâ”€â”€ entities/          # Extracted entities (CSV)
â”‚   â”œâ”€â”€ graphs/           # Graph files (pickle, graphml)
â”‚   â””â”€â”€ indexes/          # Vector indexes
â”‚
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynbâ”‚   â”œâ”€â”€ 02_entity_extraction.ipynb
â”‚   â””â”€â”€ 03_graph_analysis.ipynb
â”‚
â”œâ”€â”€ configs/              # Configuration files
â”‚   â”œâ”€â”€ pipeline.yaml    # Pipeline configuration
â”‚   â””â”€â”€ models.yaml      # Model settings
â”‚
â”œâ”€â”€ docs/                # Additional documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md  # System architecture
â”‚   â”œâ”€â”€ API.md          # API documentation
â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚
â””â”€â”€ tests/              # Unit tests
    â”œâ”€â”€ test_ingestion.py
    â”œâ”€â”€ test_extraction.py
    â””â”€â”€ test_graph.py
```

## ğŸ”§ Features

### Current Features (PoC)
- âœ… Wikipedia article ingestion via API
- âœ… Entity extraction using GLiNER + spaCy
- âœ… Customizable entity types
- âœ… Co-occurrence based relationship extraction
- âœ… Interactive graph visualization
- âœ… Basic entity validation interface- âœ… Vector similarity search
- âœ… LLM-powered Q&A with graph context

### Planned Features (Production)
- ğŸ”„ Real-time streaming pipeline
- ğŸ”„ Advanced relationship extraction
- ğŸ”„ Multi-source data ingestion
- ğŸ”„ Collaborative validation
- ğŸ”„ API endpoints
- ğŸ”„ Production graph database (Neo4j)

## ğŸ—ï¸ Architecture

The pipeline consists of five main stages:

1. **Data Ingestion**: Fetches and preprocesses Wikipedia articles
2. **Entity Extraction**: Multi-model NER using GLiNER and spaCy
3. **Graph Construction**: Builds NetworkX graph with entities and relationships
4. **Validation**: Manual review and correction through Streamlit UI
5. **Query Interface**: RAG-based Q&A using Ollama and ChromaDB

See [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) for detailed architecture documentation.

## ğŸ“Š Configuration

### Pipeline Configuration (`configs/pipeline.yaml`)
```yaml
ingestion:  batch_size: 10
  cache_enabled: true
  output_format: json

extraction:
  models:
    gliner:
      model_name: "urchade/gliner_multi_pii-v1"
      confidence_threshold: 0.5
    spacy:
      model_name: "en_core_web_sm"
      enabled_components: ["ner", "parser"]
  
  entity_types:
    default: ["PERSON", "ORGANIZATION", "LOCATION"]
    custom: ["TECHNOLOGY", "CONCEPT", "EVENT"]

graph:
  backend: "networkx"
  max_nodes: 1000
  edge_weight_threshold: 0.3
  
llm:
  provider: "ollama"
  model: "phi3:mini"
  temperature: 0.3
  context_window: 4096
```
### Environment Variables (`.env`)
```bash
# Optional: Wikipedia API settings
WIKIPEDIA_LANG=en
WIKIPEDIA_USER_AGENT=GraphRAGPoC/1.0

# Ollama settings
OLLAMA_HOST=http://localhost:11434

# ChromaDB settings
CHROMA_PERSIST_DIR=./data/indexes/chroma

# Logging
LOG_LEVEL=INFO
```

## ğŸ§ª Testing

Run the test suite:
```bash
# Run all tests
pytest

# Run specific test module
pytest tests/test_extraction.py

# Run with coverage
pytest --cov=src --cov-report=html
```

## ğŸ“ˆ Performance Metrics (PoC Targets)

| Metric | Target | Current ||--------|--------|---------|
| Articles processed | 10-20 | TBD |
| Entities extracted | 100+ | TBD |
| Graph nodes | 1000 | TBD |
| Query response time | <10s | TBD |
| Entity extraction F1 | >0.7 | TBD |

## ğŸ¤ Contributing

This is a proof of concept project. Contributions are welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- GLiNER team for zero-shot NER capabilities
- spaCy for robust NLP pipeline
- Ollama for local LLM deployment
- NetworkX for graph manipulation- PyVis for visualization

## ğŸ“š References

- [GLiNER: Generalist Model for Named Entity Recognition](https://github.com/urchade/GLiNER)
- [spaCy Documentation](https://spacy.io/)
- [NetworkX Documentation](https://networkx.org/)
- [Ollama Documentation](https://ollama.ai/)
- [ChromaDB Documentation](https://www.trychroma.com/)

## ğŸš§ Current Status

**Phase**: Proof of Concept  
**Timeline**: 6 weeks  
**Current Week**: 1 - Setup and Planning

### Development Phases
- [x] Week 0: Planning and architecture
- [ ] Week 1-2: Core pipeline development
- [ ] Week 3-4: Visualization and validation
- [ ] Week 5-6: LLM integration and testing

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---
*Last Updated: December 2024*
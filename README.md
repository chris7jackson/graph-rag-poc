# Graph RAG Pipeline - Proof of Concept

A locally-executable Graph RAG (Retrieval-Augmented Generation) pipeline that constructs knowledge graphs from Wikipedia articles and enables interactive exploration and analysis.

## ğŸ¯ Project Overview

This proof of concept demonstrates:
- **Knowledge Graph Construction**: Automated entity extraction and relationship mapping from Wikipedia articles
- **Multi-Model NER**: Combining GLiNER and spaCy for comprehensive entity recognition
- **Interactive Visualization**: Graph exploration through web-based interfaces
- **Real-time Validation**: Streamlit-based interface for graph exploration and analysis

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- 8GB RAM minimum
- 10GB free disk space (for models)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/yourusername/graph-rag-poc.git
cd graph-rag-poc
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

4. Install Ollama and download a model:
```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama service
ollama serve

# In another terminal, pull a model
ollama pull phi3:mini
```

### Basic Usage

#### Option 1: Using Makefile (Recommended)
```bash
# Complete setup
make setup

# Run the complete pipeline
make run-pipeline

# Or run individual steps
make ingest
make extract
make build

# Launch validation interface
make validate

# Clean up generated files
make clean
```

**Available Commands:**
- `make setup` - Complete setup (install + models)
- `make run-pipeline` - Run complete pipeline
- `make ingest` - Ingest Wikipedia articles
- `make extract` - Extract entities
- `make build` - Build knowledge graph
- `make validate` - Launch validation interface
- `make clean` - Clean up generated files

#### Option 2: Direct Commands
```bash
# 1. Ingest Wikipedia articles
python -m src.ingestion.wikipedia --topics "Artificial Intelligence,Machine Learning" --max-articles 10

# 2. Extract entities and build graph
python -m src.extraction.pipeline --input data/articles --output data/graphs

# 3. Launch validation interface
streamlit run src/validation/app.py
```

### Interactive Demo

Run the Jupyter notebook for a step-by-step demonstration:
```bash
jupyter notebook notebooks/demo.ipynb
```

## ğŸ“ Project Structure

```
graph-rag-poc/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package setup
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingestion/           # Wikipedia data ingestion
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ wikipedia.py     # Wikipedia API wrapper
â”‚   â”‚
â”‚   â”œâ”€â”€ extraction/          # Entity extraction pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gliner_extractor.py  # GLiNER model
â”‚   â”‚   â”œâ”€â”€ spacy_extractor.py   # spaCy NER
â”‚   â”‚   â””â”€â”€ pipeline.py          # Combined pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/               # Graph construction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ builder.py       # NetworkX graph builder
â”‚   â”‚
â”‚   â”œâ”€â”€ validation/          # Interactive validation UI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ app.py          # Streamlit application
â”‚   â”‚
â”‚   â””â”€â”€ query/              # Query interface (planned)
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ articles/          # Wikipedia articles (JSON)
â”‚   â”œâ”€â”€ entities/          # Extracted entities (JSON)
â”‚   â””â”€â”€ graphs/           # Graph files (pickle, graphml, html)
â”‚
â”œâ”€â”€ notebooks/            # Jupyter notebooks
â”‚   â””â”€â”€ demo.ipynb       # Complete pipeline demonstration
â”‚
â”œâ”€â”€ configs/              # Configuration files
â”‚   â””â”€â”€ pipeline.yaml    # Pipeline configuration
â”‚
â”œâ”€â”€ docs/                # Additional documentation
â”‚   â””â”€â”€ ARCHITECTURE.md  # System architecture
â”‚
â””â”€â”€ tests/              # Unit tests
    â””â”€â”€ test_ingestion.py
```

## ğŸ”§ Features

### âœ… Current Features (Working)
- **Wikipedia Article Ingestion**: Robust article fetching with search fallback
- **Multi-Model Entity Extraction**: GLiNER + spaCy for comprehensive NER
- **Knowledge Graph Construction**: NetworkX-based graph with entities and relationships
- **Interactive Visualization**: PyVis-based HTML visualizations
- **Streamlit Validation Interface**: Web-based graph exploration and analysis
- **Graph Statistics**: Comprehensive metrics and entity analysis
- **Data Export**: GraphML and pickle formats for interoperability

### ğŸ”„ Planned Features
- **LLM Query Interface**: Ollama-powered natural language querying
- **Advanced Relationship Extraction**: Beyond co-occurrence analysis
- **Multi-source Data Ingestion**: Support for other data sources
- **Production Database**: Neo4j integration for large-scale graphs
- **API Endpoints**: RESTful API for programmatic access

## ğŸ—ï¸ Architecture

The pipeline consists of four main stages:

1. **Data Ingestion**: Fetches Wikipedia articles with intelligent search fallback
2. **Entity Extraction**: Multi-model NER using GLiNER and spaCy
3. **Graph Construction**: Builds NetworkX graph with entities and co-occurrence relationships
4. **Validation & Exploration**: Interactive Streamlit interface for graph analysis

## ğŸ“Š Current Performance

| Metric | Value | Status |
|--------|-------|--------|
| Articles processed | 4 | âœ… Working |
| Entities extracted | 481 | âœ… Working |
| Graph nodes | 481 | âœ… Working |
| Graph edges | 2,001 | âœ… Working |
| Entity types | 16 | âœ… Working |
| Connected components | 17 | âœ… Working |

## ğŸ§ª Testing

Run the test suite:
```bash
# Run all tests
pytest

# Run specific test module
pytest tests/test_ingestion.py

# Run with coverage
pytest --cov=src --cov-report=html
```

## ğŸ“ˆ Usage Examples

### 1. Quick Start with Makefile
```bash
# Complete setup and run
make setup
make run-pipeline
make validate
```

### 2. Fetch Articles on AI Topics
```bash
python -m src.ingestion.wikipedia --topics "Deep Learning,Computer Vision,Natural Language Processing" --max-articles 5
```

### 3. Extract Entities and Build Graph
```bash
python -m src.extraction.pipeline --input data/articles --output data/graphs
```

### 4. Explore the Graph Interactively
```bash
streamlit run src/validation/app.py
```

### 5. Run the Complete Demo
```bash
jupyter notebook notebooks/demo.ipynb
```

## ğŸ” Graph Analysis Features

The Streamlit validation interface provides:

- **Graph Overview**: Statistics, density, and entity distributions
- **Entity Management**: Search, filter, and analyze entities by type
- **Relationship Analysis**: Explore connections and relationship types
- **Interactive Visualization**: Generate custom graph visualizations
- **Data Export**: Download entities and relationships as CSV

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **"No module named 'spacy'"**
   ```bash
   make setup
   # or manually:
   pip install -r requirements.txt
   python -m spacy download en_core_web_sm
   ```

2. **"Ollama port conflict"**
   ```bash
   ps aux | grep ollama
   kill -9 <PID>
   ollama serve
   ```

3. **"No articles found"**
   - Try different topic names
   - Check internet connection
   - Verify Wikipedia API access

4. **"make: command not found" (Windows)**
   - Install Make for Windows via Chocolatey: `choco install make`
   - Or use the direct commands instead of Makefile
   - Or use WSL (Windows Subsystem for Linux)

## ğŸ¤ Contributing

This is a proof of concept project. Contributions are welcome!

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- GLiNER team for zero-shot NER capabilities
- spaCy for robust NLP pipeline
- Ollama for local LLM deployment
- NetworkX for graph manipulation
- PyVis for interactive visualizations
- Streamlit for web interface framework

## ğŸ“š References

- [GLiNER: Generalist Model for Named Entity Recognition](https://github.com/urchade/GLiNER)
- [spaCy Documentation](https://spacy.io/)
- [NetworkX Documentation](https://networkx.org/)
- [Ollama Documentation](https://ollama.ai/)
- [PyVis Documentation](https://pyvis.readthedocs.io/)
- [Streamlit Documentation](https://docs.streamlit.io/)

## ğŸš§ Current Status

**Phase**: Proof of Concept âœ…  
**Status**: Fully Functional  
**Last Updated**: August 2024

### âœ… Completed Features
- [x] Wikipedia article ingestion with search fallback
- [x] Multi-model entity extraction (GLiNER + spaCy)
- [x] Knowledge graph construction
- [x] Interactive Streamlit validation interface
- [x] Graph visualization and analysis
- [x] Comprehensive testing and error handling

### ğŸ”„ Next Steps
- [ ] LLM query interface integration
- [ ] Advanced relationship extraction
- [ ] Multi-source data ingestion
- [ ] Production database integration

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---
*Last Updated: August 2024*
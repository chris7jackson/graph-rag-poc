{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph RAG Pipeline - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the complete Graph RAG pipeline from data ingestion to querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.ingestion import WikipediaIngester\n",
    "from src.extraction import CombinedExtractor\n",
    "from src.graph import GraphBuilder\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Ingestion\n",    "\n",
    "Fetch Wikipedia articles on AI-related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ingester\n",
    "ingester = WikipediaIngester()\n",
    "\n",
    "# Define topics\n",
    "topics = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"Machine Learning\",\n",
    "    \"Natural Language Processing\"\n",
    "]\n",
    "\n",
    "# Fetch articles\n",
    "articles = ingester.fetch_articles(topics, max_articles=3)\n",
    "print(f\"Fetched {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Entity Extraction"
   ]
  },
  {
   "cell_type": "code",   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = CombinedExtractor()\n",
    "\n",
    "# Process articles\n",
    "results = extractor.process_articles('../data/articles')\n",
    "\n",
    "# Display statistics\n",
    "total_entities = sum(r['stats']['total_entities'] for r in results)\n",
    "print(f\"Extracted {total_entities} total entities\")\n",
    "print(f\"Average entities per article: {total_entities/len(results):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize graph builder\n",
    "builder = GraphBuilder()\n",
    "\n",
    "# Load extraction results\n",    "extraction_files = list(Path('../data/entities').glob('*_entities.json'))\n",
    "\n",
    "# Build graph\n",
    "graph = builder.build_from_extractions(extraction_files)\n",
    "\n",
    "# Save graph\n",
    "builder.save_graph('demo_graph')\n",
    "\n",
    "print(f\"Graph built with {len(graph.nodes)} nodes and {len(graph.edges)} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic graph statistics\n",
    "print(\"Graph Statistics:\")\n",
    "print(f\"  Density: {nx.density(graph):.4f}\")\n",
    "print(f\"  Average degree: {sum(dict(graph.degree()).values()) / len(graph.nodes):.2f}\")\n",
    "print(f\"  Number of components: {nx.number_weakly_connected_components(graph)}\")"
   ]
  },
  {
   "cell_type": "code",   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top entities by centrality\n",
    "centrality = nx.degree_centrality(graph)\n",
    "top_10 = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Create DataFrame for display\n",
    "df_top = pd.DataFrame([\n",
    "    {\n",
    "        'Entity': graph.nodes[node].get('text', node),\n",
    "        'Type': graph.nodes[node].get('label', 'UNKNOWN'),\n",
    "        'Centrality': score,\n",
    "        'Connections': graph.degree(node)\n",
    "    }\n",
    "    for node, score in top_10\n",
    "])\n",
    "\n",
    "df_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualization\n",
    "builder.visualize('demo_graph.html', max_nodes=50)\n",
    "print(\"Visualization saved to ../data/graphs/demo_graph.html\")\n",
    "print(\"Open the HTML file in your browser to explore the graph interactively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple matplotlib visualization of entity type distribution\n",
    "entity_types = {}\n",
    "for node, data in graph.nodes(data=True):\n",
    "    label = data.get('label', 'UNKNOWN')\n",
    "    entity_types[label] = entity_types.get(label, 0) + 1\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(entity_types.keys(), entity_types.values())\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Entity Type Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Validation**: Run `streamlit run ../src/validation/app.py` to launch the validation interface\n",
    "2. **Query Interface**: Implement LLM integration with Ollama for natural language queries\n",
    "3. **Scale Up**: Process more articles and build a larger knowledge graph\n",
    "4. **Custom Entities**: Add domain-specific entity types using GLiNER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
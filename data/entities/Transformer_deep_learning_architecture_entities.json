{
  "doc_id": "Transformer (deep learning architecture)",
  "entities": [
    {
      "text": "2017",
      "label": "DATE",
      "start": 885,
      "end": 889,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Attention Is All You Need",
      "label": "WORK_OF_ART",
      "start": 897,
      "end": 922,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Google",
      "label": "ORG",
      "start": 942,
      "end": 948,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "first",
      "label": "ORDINAL",
      "start": 968,
      "end": 973,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "BERT",
      "label": "ORG",
      "start": 1392,
      "end": 1396,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "many years",
      "label": "DATE",
      "start": 1499,
      "end": 1509,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Elman",
      "label": "ORG",
      "start": 1638,
      "end": 1643,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "1990",
      "label": "DATE",
      "start": 1653,
      "end": 1657,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "1995",
      "label": "DATE",
      "start": 1947,
      "end": 1951,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "RNN",
      "label": "ORG",
      "start": 1957,
      "end": 1960,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "One",
      "label": "CARDINAL",
      "start": 2092,
      "end": 2095,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Transformers",
      "label": "ORG",
      "start": 2445,
      "end": 2457,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "1992",
      "label": "DATE",
      "start": 2847,
      "end": 2851,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "two",
      "label": "CARDINAL",
      "start": 2945,
      "end": 2948,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "1981",
      "label": "DATE",
      "start": 2997,
      "end": 3001,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "the early 2010s",
      "label": "DATE",
      "start": 3360,
      "end": 3375,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2014",
      "label": "DATE",
      "start": 3476,
      "end": 3480,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "380M",
      "label": "CARDINAL",
      "start": 3484,
      "end": 3488,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "130M",
      "label": "CARDINAL",
      "start": 3796,
      "end": 3800,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "RNNsearch",
      "label": "ORG",
      "start": 4619,
      "end": 4628,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2016",
      "label": "DATE",
      "start": 5221,
      "end": 5225,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Google Translate",
      "label": "ORG",
      "start": 5227,
      "end": 5243,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Google Neural Machine Translation",
      "label": "ORG",
      "start": 5260,
      "end": 5293,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "8",
      "label": "CARDINAL",
      "start": 5449,
      "end": 5450,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "nine months",
      "label": "DATE",
      "start": 5489,
      "end": 5500,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "ten years",
      "label": "DATE",
      "start": 5570,
      "end": 5579,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "SOTA",
      "label": "ORG",
      "start": 5977,
      "end": 5981,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Jakob Uszkoreit",
      "label": "PERSON",
      "start": 6087,
      "end": 6102,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Hans Uszkoreit",
      "label": "PERSON",
      "start": 6319,
      "end": 6333,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "the same year",
      "label": "DATE",
      "start": 6390,
      "end": 6403,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "100M",
      "label": "CARDINAL",
      "start": 6520,
      "end": 6524,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "AI",
      "label": "ORG",
      "start": 7120,
      "end": 7122,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "spring 2017",
      "label": "DATE",
      "start": 7147,
      "end": 7158,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Wikipedia",
      "label": "ORG",
      "start": 7330,
      "end": 7339,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2018",
      "label": "DATE",
      "start": 7489,
      "end": 7493,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "a bi-directional",
      "label": "DATE",
      "start": 7499,
      "end": 7515,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Transformer",
      "label": "ORG",
      "start": 7683,
      "end": 7694,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2019 October",
      "label": "DATE",
      "start": 7705,
      "end": 7717,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2020",
      "label": "DATE",
      "start": 7775,
      "end": 7779,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPT",
      "label": "ORG",
      "start": 7923,
      "end": 7926,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2022",
      "label": "DATE",
      "start": 8022,
      "end": 8026,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPT-3",
      "label": "ORG",
      "start": 8047,
      "end": 8052,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2021",
      "label": "DATE",
      "start": 8425,
      "end": 8429,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Stable Diffusion 3 (",
      "label": "ORG",
      "start": 8432,
      "end": 8452,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Sora",
      "label": "PERSON",
      "start": 8463,
      "end": 8467,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2024",
      "label": "DATE",
      "start": 8469,
      "end": 8473,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "0",
      "label": "CARDINAL",
      "start": 8978,
      "end": 8979,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2%",
      "label": "PERCENT",
      "start": 9059,
      "end": 9061,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "The Pile",
      "label": "ORG",
      "start": 9567,
      "end": 9575,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "T5",
      "label": "NORP",
      "start": 9754,
      "end": 9756,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "last week",
      "label": "DATE",
      "start": 10046,
      "end": 10055,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "3",
      "label": "CARDINAL",
      "start": 10621,
      "end": 10622,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\displaystyle",
      "label": "PERSON",
      "start": 11495,
      "end": 11508,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "{t\\in {\\text{masked",
      "label": "PERSON",
      "start": 11530,
      "end": 11549,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "second",
      "label": "ORDINAL",
      "start": 11969,
      "end": 11975,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Tokenizers",
      "label": "PRODUCT",
      "start": 12724,
      "end": 12734,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Un",
      "label": "ORG",
      "start": 13175,
      "end": 13177,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "byte",
      "label": "ORG",
      "start": 14556,
      "end": 14560,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "WordPiece",
      "label": "ORG",
      "start": 14576,
      "end": 14585,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "SentencePiece",
      "label": "ORG",
      "start": 14591,
      "end": 14604,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Equivalently",
      "label": "PERSON",
      "start": 14696,
      "end": 14708,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "1",
      "label": "CARDINAL",
      "start": 15086,
      "end": 15087,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "UnEmbed",
      "label": "PERSON",
      "start": 16834,
      "end": 16841,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "x)=\\mathrm",
      "label": "ORG",
      "start": 16844,
      "end": 16854,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "xW+b",
      "label": "NORP",
      "start": 16866,
      "end": 16870,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "W\n      \n    \n    ",
      "label": "PERSON",
      "start": 17299,
      "end": 17317,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2",
      "label": "CARDINAL",
      "start": 18514,
      "end": 18515,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "d/2-1\\",
      "label": "ORG",
      "start": 19158,
      "end": 19164,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "10000",
      "label": "CARDINAL",
      "start": 19916,
      "end": 19921,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "j}c_{j}f(t+\\Delta",
      "label": "PERSON",
      "start": 23088,
      "end": 23105,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "{j}c_{j}\\,\\mathrm",
      "label": "PERSON",
      "start": 23125,
      "end": 23142,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "fed",
      "label": "ORG",
      "start": 23484,
      "end": 23487,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Feedforward",
      "label": "PERSON",
      "start": 25147,
      "end": 25158,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "FFN",
      "label": "ORG",
      "start": 25197,
      "end": 25200,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "W^{(2",
      "label": "PRODUCT",
      "start": 26207,
      "end": 26212,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "ReLU",
      "label": "PRODUCT",
      "start": 26705,
      "end": 26709,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPT-2",
      "label": "GPE",
      "start": 26916,
      "end": 26921,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "4",
      "label": "CARDINAL",
      "start": 26982,
      "end": 26983,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "three",
      "label": "CARDINAL",
      "start": 27462,
      "end": 27467,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\text{seq",
      "label": "PRODUCT",
      "start": 28178,
      "end": 28187,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "K",
      "label": "ORG",
      "start": 35330,
      "end": 35331,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm",
      "label": "PERSON",
      "start": 35332,
      "end": 35376,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "key}}=\\ell",
      "label": "NORP",
      "start": 36842,
      "end": 36852,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "{\\text{seq",
      "label": "PERSON",
      "start": 36854,
      "end": 36864,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPT-2",
      "label": "PERSON",
      "start": 42556,
      "end": 42561,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "768",
      "label": "CARDINAL",
      "start": 42745,
      "end": 42748,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "12",
      "label": "CARDINAL",
      "start": 42846,
      "end": 42848,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "64",
      "label": "CARDINAL",
      "start": 42946,
      "end": 42948,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "64=768",
      "label": "CARDINAL",
      "start": 43157,
      "end": 43163,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "64)\\times 768",
      "label": "CARDINAL",
      "start": 43533,
      "end": 43546,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "−",
      "label": "CARDINAL",
      "start": 45037,
      "end": 45038,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "V)={\\text{softmax}}\\left(M+{\\frac",
      "label": "PRODUCT",
      "start": 46489,
      "end": 46522,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix",
      "label": "ORG",
      "start": 48647,
      "end": 48716,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "zero",
      "label": "CARDINAL",
      "start": 48945,
      "end": 48949,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "XLNet",
      "label": "ORG",
      "start": 49004,
      "end": 49009,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "P\n      \n    \n    {\\displaystyle P}\n  \n ",
      "label": "ORG",
      "start": 49311,
      "end": 49351,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "EncoderLayer",
      "label": "ORG",
      "start": 51394,
      "end": 51406,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "MultiheadAttention",
      "label": "ORG",
      "start": 51839,
      "end": 51857,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Schematically",
      "label": "PERSON",
      "start": 56119,
      "end": 56132,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "DecoderLayer",
      "label": "PRODUCT",
      "start": 56639,
      "end": 56651,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\begin{aligned}H'&={\\text{MaskedMultiheadAttention}}(H",
      "label": "CARDINAL",
      "start": 57405,
      "end": 57459,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "LN",
      "label": "GPE",
      "start": 58851,
      "end": 58853,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "LayerNorm",
      "label": "ORG",
      "start": 59436,
      "end": 59445,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "post-LN",
      "label": "EVENT",
      "start": 59536,
      "end": 59543,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "{Sublayer",
      "label": "PERSON",
      "start": 60017,
      "end": 60026,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "pre-LN",
      "label": "DATE",
      "start": 60321,
      "end": 60327,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "← encoder.tokenizer(t_e",
      "label": "LAW",
      "start": 61430,
      "end": 61453,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "← encoder.embedding(z_e[t]",
      "label": "PERSON",
      "start": 61498,
      "end": 61524,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "←",
      "label": "ORG",
      "start": 61613,
      "end": 61614,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "z_e_copy ← copy(z_e",
      "label": "PERSON",
      "start": 61663,
      "end": 61682,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "layer.multihead_attention(z_e",
      "label": "PRODUCT",
      "start": 61771,
      "end": 61800,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "←",
      "label": "PERSON",
      "start": 62163,
      "end": 62164,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "← decoder.tokenizer(t_d",
      "label": "LAW",
      "start": 62217,
      "end": 62240,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "← decoder.embedding(z_d[t])",
      "label": "PERSON",
      "start": 62285,
      "end": 62312,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "← layer.multihead_attention(z_d",
      "label": "ORG",
      "start": 62868,
      "end": 62899,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "third",
      "label": "ORDINAL",
      "start": 63005,
      "end": 63010,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "just two",
      "label": "CARDINAL",
      "start": 64190,
      "end": 64198,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Chinchilla",
      "label": "NORP",
      "start": 64376,
      "end": 64386,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Llama",
      "label": "LOC",
      "start": 66589,
      "end": 66594,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "PaLM",
      "label": "ORG",
      "start": 66606,
      "end": 66610,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "SwiGLU",
      "label": "ORG",
      "start": 66616,
      "end": 66622,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPT-1",
      "label": "ORG",
      "start": 66629,
      "end": 66634,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GELU",
      "label": "ORG",
      "start": 66649,
      "end": 66653,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Gated Linear Units",
      "label": "ORG",
      "start": 66723,
      "end": 66741,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "CapsuleNorm ScaleNorm",
      "label": "PRODUCT",
      "start": 66962,
      "end": 66983,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "FixNorm",
      "label": "ORG",
      "start": 66988,
      "end": 66995,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "RoPE",
      "label": "GPE",
      "start": 67454,
      "end": 67458,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "−",
      "label": "TIME",
      "start": 70980,
      "end": 70981,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "m\\theta &-\\sin m\\theta",
      "label": "ORG",
      "start": 72218,
      "end": 72240,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "m\\theta &\\cos",
      "label": "ORG",
      "start": 72248,
      "end": 72261,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin",
      "label": "PERSON",
      "start": 72270,
      "end": 72397,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\\\x_{m}^{(2)}\\cos",
      "label": "MONEY",
      "start": 72406,
      "end": 72423,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "+x_{m}^{(1)}\\sin m\\theta",
      "label": "GPE",
      "start": 72432,
      "end": 72456,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Equivalently",
      "label": "GPE",
      "start": 72478,
      "end": 72490,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2n",
      "label": "CARDINAL",
      "start": 73630,
      "end": 73632,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "RoPE",
      "label": "ORG",
      "start": 73661,
      "end": 73665,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "m{\\big",
      "label": "GPE",
      "start": 75131,
      "end": 75137,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Linear Biases",
      "label": "PERSON",
      "start": 75376,
      "end": 75389,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "ALiBi",
      "label": "ORG",
      "start": 75589,
      "end": 75594,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "B\n      \n    \n    ",
      "label": "PRODUCT",
      "start": 77113,
      "end": 77131,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix",
      "label": "ORG",
      "start": 79060,
      "end": 79115,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Relative Position Encodings",
      "label": "ORG",
      "start": 80148,
      "end": 80175,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Toeplitz",
      "label": "GPE",
      "start": 81635,
      "end": 81643,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "TensorFlow",
      "label": "PRODUCT",
      "start": 82459,
      "end": 82469,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "PyTorch",
      "label": "PRODUCT",
      "start": 82474,
      "end": 82481,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "KV",
      "label": "ORG",
      "start": 82825,
      "end": 82827,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "FlashAttention",
      "label": "ORG",
      "start": 83336,
      "end": 83350,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPU",
      "label": "ORG",
      "start": 83436,
      "end": 83439,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "up to 230",
      "label": "CARDINAL",
      "start": 83964,
      "end": 83973,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "A100",
      "label": "CARDINAL",
      "start": 83986,
      "end": 83990,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2x",
      "label": "CARDINAL",
      "start": 84011,
      "end": 84013,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "256",
      "label": "CARDINAL",
      "start": 84288,
      "end": 84291,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "MQA",
      "label": "ORG",
      "start": 84319,
      "end": 84322,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Benchmarks",
      "label": "ORG",
      "start": 84359,
      "end": 84369,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "9x",
      "label": "CARDINAL",
      "start": 84448,
      "end": 84450,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "PyTorch",
      "label": "PERSON",
      "start": 84502,
      "end": 84509,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "H100",
      "label": "PRODUCT",
      "start": 84574,
      "end": 84578,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Multi-Query Attention",
      "label": "ORG",
      "start": 84620,
      "end": 84641,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "MQA",
      "label": "PERSON",
      "start": 87606,
      "end": 87609,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GQA",
      "label": "ORG",
      "start": 87613,
      "end": 87616,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Multihead Attention",
      "label": "ORG",
      "start": 87648,
      "end": 87667,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Multihead Latent Attention",
      "label": "ORG",
      "start": 87711,
      "end": 87737,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "512",
      "label": "CARDINAL",
      "start": 88584,
      "end": 88587,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "512T_{\\text{GPT-3",
      "label": "CARDINAL",
      "start": 89213,
      "end": 89230,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "four",
      "label": "CARDINAL",
      "start": 89877,
      "end": 89881,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "4T_{\\text{GPT-3",
      "label": "CARDINAL",
      "start": 90874,
      "end": 90889,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Multi-Token Prediction",
      "label": "FAC",
      "start": 92951,
      "end": 92973,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "just one",
      "label": "CARDINAL",
      "start": 93307,
      "end": 93315,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Swin Transformer",
      "label": "PERSON",
      "start": 93581,
      "end": 93597,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "SepTr",
      "label": "NORP",
      "start": 93697,
      "end": 93702,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Range Arena",
      "label": "PERSON",
      "start": 93763,
      "end": 93774,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Reformer",
      "label": "PERSON",
      "start": 94272,
      "end": 94280,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "BigBird",
      "label": "ORG",
      "start": 94909,
      "end": 94916,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Random Feature Attention",
      "label": "ORG",
      "start": 95354,
      "end": 95378,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Fourier",
      "label": "PERSON",
      "start": 95391,
      "end": 95398,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "1}{\\sqrt",
      "label": "DATE",
      "start": 96256,
      "end": 96264,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\sin \\langle",
      "label": "PERSON",
      "start": 96365,
      "end": 96377,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "−\n            \n              \n                \n                  ",
      "label": "FAC",
      "start": 97173,
      "end": 97238,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\approx",
      "label": "GPE",
      "start": 99802,
      "end": 99809,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "x),e^{\\|y\\|^{2}/2\\sigma",
      "label": "NORP",
      "start": 99853,
      "end": 99876,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "\\text{Attention}}(q",
      "label": "GPE",
      "start": 102814,
      "end": 102833,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "V)={\\text{softmax}}\\left({\\frac {",
      "label": "PERSON",
      "start": 102836,
      "end": 102869,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "i}e^{\\|k_{i}\\|^{2}/2\\sigma",
      "label": "NORP",
      "start": 102949,
      "end": 102975,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "k_{i})v_{i}^{T}}{\\varphi",
      "label": "PERSON",
      "start": 102990,
      "end": 103014,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "k_{i})v_{i}^{T",
      "label": "ORG",
      "start": 103735,
      "end": 103749,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Performer",
      "label": "PERSON",
      "start": 104923,
      "end": 104932,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Gram-Schmidt",
      "label": "PERSON",
      "start": 105506,
      "end": 105518,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2022",
      "label": "CARDINAL",
      "start": 105770,
      "end": 105774,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "only 0.03%",
      "label": "PERCENT",
      "start": 105861,
      "end": 105871,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Vision",
      "label": "ORG",
      "start": 106171,
      "end": 106177,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Conformer",
      "label": "ORG",
      "start": 106370,
      "end": 106379,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Whisper",
      "label": "PERSON",
      "start": 106390,
      "end": 106397,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Perceivers",
      "label": "GPE",
      "start": 106649,
      "end": 106659,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Transformers",
      "label": "NORP",
      "start": 106677,
      "end": 106689,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Phenaki",
      "label": "PERSON",
      "start": 106797,
      "end": 106804,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2023",
      "label": "DATE",
      "start": 106806,
      "end": 106810,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Muse",
      "label": "PERSON",
      "start": 106817,
      "end": 106821,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Phenaki",
      "label": "ORG",
      "start": 107497,
      "end": 107504,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "NLP",
      "label": "ORG",
      "start": 107754,
      "end": 107757,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPT-2",
      "label": "ORG",
      "start": 107795,
      "end": 107800,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "GPT-4",
      "label": "GPE",
      "start": 107809,
      "end": 107814,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Gemini",
      "label": "GPE",
      "start": 107816,
      "end": 107822,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Claude",
      "label": "ORG",
      "start": 107836,
      "end": 107842,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Grok",
      "label": "PERSON",
      "start": 107850,
      "end": 107854,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "NER",
      "label": "ORG",
      "start": 108140,
      "end": 108143,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Minimax",
      "label": "PERSON",
      "start": 108504,
      "end": 108511,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "2895",
      "label": "DATE",
      "start": 108551,
      "end": 108555,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Google AI",
      "label": "ORG",
      "start": 108902,
      "end": 108911,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "T5",
      "label": "PRODUCT",
      "start": 108978,
      "end": 108980,
      "confidence": 1.0,
      "extractor": "spacy"
    },
    {
      "text": "Notes",
      "label": "PRODUCT",
      "start": 109060,
      "end": 109065,
      "confidence": 1.0,
      "extractor": "spacy"
    }
  ],
  "relationships": [
    [
      "lookup",
      "from",
      "via"
    ],
    [
      "scope",
      "of",
      "within"
    ],
    [
      "scope",
      "with",
      "within"
    ],
    [
      "tokens",
      "via",
      "with"
    ],
    [
      "advantage",
      "of",
      "have"
    ],
    [
      "time",
      "than",
      "requiring"
    ],
    [
      "architectures",
      "as",
      "than"
    ],
    [
      "researchers",
      "at",
      "by"
    ],
    [
      "improvement",
      "over",
      "as"
    ],
    [
      "architectures",
      "for",
      "over"
    ],
    [
      "development",
      "of",
      "to"
    ],
    [
      "development",
      "as",
      "to"
    ],
    [
      "information",
      "from",
      "propagate"
    ],
    [
      "end",
      "of",
      "at"
    ],
    [
      "sentence",
      "without",
      "of"
    ],
    [
      "information",
      "about",
      "without"
    ],
    [
      "learning",
      "of",
      "allowing"
    ],
    [
      "outputs",
      "of",
      "multiply"
    ],
    [
      "modelling",
      "until",
      "for"
    ],
    [
      "publication",
      "of",
      "until"
    ],
    [
      "time",
      "from",
      "at"
    ],
    [
      "parallel",
      "over",
      "in"
    ],
    [
      "size",
      "of",
      "in"
    ],
    [
      "matrix",
      "for",
      "compute"
    ],
    [
      "One",
      "of",
      "has"
    ],
    [
      "keys",
      "for",
      "generate"
    ],
    [
      "changes",
      "of",
      "computing"
    ],
    [
      "answers",
      "to",
      "computes"
    ],
    [
      "idea",
      "of",
      "with"
    ],
    [
      "model",
      "for",
      "uses"
    ],
    [
      "sequence",
      "of",
      "takes"
    ],
    [
      "sequence",
      "of",
      "into"
    ],
    [
      "information",
      "about",
      "retains"
    ],
    [
      "seq2seq",
      "for",
      "to"
    ],
    [
      "problem",
      "of",
      "solve"
    ],
    [
      "quality",
      "than",
      "had"
    ],
    [
      "models",
      "with",
      "suffered"
    ],
    [
      "models",
      "including",
      "suffered"
    ],
    [
      "issue",
      "with",
      "from"
    ],
    [
      "order",
      "of",
      "with"
    ],
    [
      "parameters",
      "than",
      "of"
    ],
    [
      "One",
      "of",
      "suspected"
    ],
    [
      "attention",
      "without",
      "be"
    ],
    [
      "focus",
      "of",
      "was"
    ],
    [
      "seq2seq",
      "for",
      "improving"
    ],
    [
      "introduction",
      "of",
      "to"
    ],
    [
      "use",
      "of",
      "due"
    ],
    [
      "one",
      "of",
      "applied"
    ],
    [
      "variation",
      "of",
      "applied"
    ],
    [
      "line",
      "of",
      "upon"
    ],
    [
      "line",
      "from",
      "upon"
    ],
    [
      "bag",
      "of",
      "from"
    ],
    [
      "series",
      "of",
      "became"
    ],
    [
      "chatbot",
      "based",
      "became"
    ],
    [
      "boom",
      "around",
      "triggering"
    ],
    [
      "text",
      "including",
      "beyond"
    ],
    [
      "developments",
      "in",
      "stimulated"
    ],
    [
      "data",
      "like",
      "analyse"
    ],
    [
      "relevance",
      "between",
      "calculating"
    ],
    [
      "context",
      "within",
      "understand"
    ],
    [
      "Methods",
      "for",
      "had"
    ],
    [
      "part",
      "of",
      "for"
    ],
    [
      "number",
      "of",
      "of"
    ],
    [
      "tuning",
      "on",
      "by"
    ],
    [
      "Tasks",
      "for",
      "include"
    ],
    [
      "number",
      "of",
      "answering"
    ],
    [
      "translation",
      "between",
      "Thank"
    ],
    [
      "acceptability",
      "of",
      "judging"
    ],
    [
      "each",
      "of",
      "is"
    ],
    [
      "speakers",
      "of",
      "for"
    ],
    [
      "generations",
      "of",
      "for"
    ],
    [
      "architecture",
      "as",
      "of"
    ],
    [
      "context",
      "of",
      "in"
    ],
    [
      "function",
      "for",
      "is"
    ],
    [
      "distribution",
      "for",
      "produces"
    ],
    [
      "function",
      "for",
      "is"
    ],
    [
      "token",
      "of",
      "predicts"
    ],
    [
      "function",
      "for",
      "is"
    ],
    [
      "transformations",
      "on",
      "carry"
    ],
    [
      "distribution",
      "over",
      "to"
    ],
    [
      "matrix",
      "on",
      "by"
    ],
    [
      "conversion",
      "between",
      "doing"
    ],
    [
      "set",
      "of",
      "is"
    ],
    [
      "representation",
      "of",
      "multiplies"
    ],
    [
      "sequence",
      "of",
      "producing"
    ],
    [
      "distribution",
      "over",
      "into"
    ],
    [
      "positions",
      "of",
      "of"
    ],
    [
      "information",
      "about",
      "with"
    ],
    [
      "bias",
      "towards",
      "induces"
    ],
    [
      "order",
      "of",
      "towards"
    ],
    [
      "function",
      "of",
      "as"
    ],
    [
      "function",
      "of",
      "as"
    ],
    [
      "reason",
      "for",
      "is"
    ],
    [
      "encoding",
      "of",
      "find"
    ],
    [
      "sum",
      "of",
      "find"
    ],
    [
      "locations",
      "of",
      "of"
    ],
    [
      "sum",
      "of",
      "create"
    ],
    [
      "weights",
      "on",
      "create"
    ],
    [
      "layer",
      "after",
      "tokens"
    ],
    [
      "purpose",
      "of",
      "is"
    ],
    [
      "representations",
      "of",
      "create"
    ],
    [
      "information",
      "from",
      "corresponds"
    ],
    [
      "information",
      "via",
      "corresponds"
    ],
    [
      "output",
      "of",
      "incorporating"
    ],
    [
      "attention",
      "for",
      "tokens"
    ],
    [
      "information",
      "among",
      "for"
    ],
    [
      "network",
      "for",
      "have"
    ],
    [
      "processing",
      "of",
      "for"
    ],
    [
      "most",
      "of",
      "contain"
    ],
    [
      "modules",
      "in",
      "are"
    ],
    [
      "matrix",
      "of",
      "is"
    ],
    [
      "root",
      "of",
      "by"
    ],
    [
      "dimension",
      "of",
      "of"
    ],
    [
      "output",
      "of",
      "is"
    ],
    [
      "output",
      "for",
      "is"
    ],
    [
      "vectors",
      "of",
      "of"
    ],
    [
      "attention",
      "from",
      "weighted"
    ],
    [
      "attention",
      "to",
      "weighted"
    ],
    [
      "training",
      "due",
      "for"
    ],
    [
      "attention",
      "as",
      "represent"
    ],
    [
      "each",
      "of",
      "over"
    ],
    [
      "rows",
      "of",
      "of"
    ],
    [
      "number",
      "of",
      "is"
    ],
    [
      "number",
      "in",
      "is"
    ],
    [
      "dimension",
      "of",
      "is"
    ],
    [
      "layer",
      "in",
      "has"
    ],
    [
      "definitions",
      "of",
      "for"
    ],
    [
      "matrix",
      "in",
      "determines"
    ],
    [
      "combination",
      "with",
      "in"
    ],
    [
      "part",
      "of",
      "with"
    ],
    [
      "scope",
      "of",
      "expand"
    ],
    [
      "dependencies",
      "in",
      "capture"
    ],
    [
      "calculation",
      "of",
      "to"
    ],
    [
      "properties",
      "of",
      "ensures"
    ],
    [
      "calculation",
      "for",
      "at"
    ],
    [
      "access",
      "to",
      "have"
    ],
    [
      "access",
      "at",
      "have"
    ],
    [
      "0",
      "at",
      "adding"
    ],
    [
      "example",
      "of",
      "As"
    ],
    [
      "use",
      "of",
      "of"
    ],
    [
      "masks",
      "of",
      "considers"
    ],
    [
      "input",
      "as",
      "takes"
    ],
    [
      "sequence",
      "of",
      "as"
    ],
    [
      "sequence",
      "of",
      "produce"
    ],
    [
      "layer",
      "for",
      "applies"
    ],
    [
      "row",
      "of",
      "to"
    ],
    [
      "sequence",
      "of",
      "takes"
    ],
    [
      "sequence",
      "of",
      "producing"
    ],
    [
      "information",
      "of",
      "takes"
    ],
    [
      "vectors",
      "of",
      "to"
    ],
    [
      "H",
      "′",
      "have"
    ],
    [
      "probabilities",
      "over",
      "produce"
    ],
    [
      "encoder",
      "of",
      "use"
    ],
    [
      "token",
      "in",
      "predict"
    ],
    [
      "use",
      "of",
      "makes"
    ],
    [
      "token",
      "in",
      "predict"
    ],
    [
      "points",
      "of",
      "are"
    ],
    [
      "transformation",
      "of",
      "of"
    ],
    [
      "gradient",
      "of",
      "is"
    ],
    [
      "output",
      "of",
      "is"
    ],
    [
      "output",
      "of",
      "is"
    ],
    [
      "←",
      "for",
      "do"
    ],
    [
      "←",
      "in",
      "do"
    ],
    [
      "encoder.positional_embedding(t",
      "for",
      "do"
    ],
    [
      "encoder.positional_embedding(t",
      "in",
      "do"
    ],
    [
      "layer.multihead_attention(z_e",
      "for",
      "do"
    ],
    [
      "layer.multihead_attention(z_e",
      "in",
      "do"
    ],
    [
      "←",
      "for",
      "do"
    ],
    [
      "←",
      "in",
      "do"
    ],
    [
      "decoder.embedding(z_d[t",
      "for",
      "do"
    ],
    [
      "decoder.embedding(z_d[t",
      "in",
      "do"
    ],
    [
      "←",
      "for",
      "do"
    ],
    [
      "←",
      "in",
      "do"
    ],
    [
      "z_d_copy",
      "for",
      "do"
    ],
    [
      "z_d_copy",
      "in",
      "do"
    ],
    [
      "layer.multihead_attention(z_d",
      "for",
      "do"
    ],
    [
      "layer.multihead_attention(z_d",
      "in",
      "do"
    ],
    [
      "z_d_copy",
      "for",
      "do"
    ],
    [
      "z_d_copy",
      "in",
      "do"
    ],
    [
      "layer.feedforward(z_d",
      "for",
      "do"
    ],
    [
      "layer.feedforward(z_d",
      "in",
      "do"
    ],
    [
      "sequence",
      "of",
      "into"
    ],
    [
      "models",
      "in",
      "are"
    ],
    [
      "sublayers",
      "per",
      "with"
    ],
    [
      "improvements",
      "as",
      "have"
    ],
    [
      "location",
      "of",
      "changing"
    ],
    [
      "models",
      "in",
      "are"
    ],
    [
      "mask",
      "of",
      "has"
    ],
    [
      "combination",
      "with",
      "in"
    ],
    [
      "Units",
      "in",
      "with"
    ],
    [
      "methods",
      "than",
      "use"
    ],
    [
      "signal",
      "to",
      "provides"
    ],
    [
      "list",
      "of",
      "considering"
    ],
    [
      "list",
      "of",
      "For"
    ],
    [
      "sequence",
      "of",
      "by"
    ],
    [
      "pair",
      "of",
      "to"
    ],
    [
      "benefit",
      "of",
      "is"
    ],
    [
      "product",
      "between",
      "depends"
    ],
    [
      "encoder",
      "on",
      "for"
    ],
    [
      "bottom",
      "of",
      "into"
    ],
    [
      "frameworks",
      "as",
      "in"
    ],
    [
      "inference",
      "as",
      "for"
    ],
    [
      "saving",
      "in",
      "is"
    ],
    [
      "interactions",
      "as",
      "for"
    ],
    [
      "multiplications",
      "in",
      "performs"
    ],
    [
      "cache",
      "of",
      "within"
    ],
    [
      "management",
      "of",
      "by"
    ],
    [
      "copying",
      "between",
      "by"
    ],
    [
      "page",
      "on",
      "See"
    ],
    [
      "softmax",
      "for",
      "on"
    ],
    [
      "demand",
      "for",
      "to"
    ],
    [
      "enhancements",
      "in",
      "offers"
    ],
    [
      "increase",
      "over",
      "enabling"
    ],
    [
      "advancements",
      "in",
      "include"
    ],
    [
      "reduction",
      "of",
      "include"
    ],
    [
      "support",
      "for",
      "added"
    ],
    [
      "implementation",
      "in",
      "than"
    ],
    [
      "optimization",
      "for",
      "include"
    ],
    [
      "hardware",
      "like",
      "for"
    ],
    [
      "effect",
      "on",
      "has"
    ],
    [
      "each",
      "of",
      "shares"
    ],
    [
      "number",
      "of",
      "with"
    ],
    [
      "execution",
      "in",
      "to"
    ],
    [
      "factor",
      "in",
      "is"
    ],
    [
      "models",
      "like",
      "have"
    ],
    [
      "size",
      "of",
      "with"
    ],
    [
      "decoding",
      "with",
      "with"
    ],
    [
      "guess",
      "for",
      "had"
    ],
    [
      "values",
      "of",
      "for"
    ],
    [
      "all",
      "of",
      "verify"
    ],
    [
      "run",
      "of",
      "in"
    ],
    [
      "likelihood",
      "in",
      "with"
    ],
    [
      "steps",
      "into",
      "for"
    ],
    [
      "accuracy",
      "for",
      "trades"
    ],
    [
      "behavior",
      "of",
      "comparing"
    ],
    [
      "both",
      "of",
      "scales"
    ],
    [
      "load",
      "from",
      "reduces"
    ],
    [
      "size",
      "of",
      "in"
    ],
    [
      "advantages",
      "of",
      "retaining"
    ],
    [
      "key",
      "to",
      "linking"
    ],
    [
      "φ",
      "⟨",
      "uses"
    ],
    [
      "φ",
      "⟨",
      "uses"
    ],
    [
      "φ",
      "\\langle",
      "uses"
    ],
    [
      "choice",
      "of",
      "satisfy"
    ],
    [
      "version",
      "of",
      "obtain"
    ],
    [
      "\\displaystyle",
      "\\text{Attention}}(Q",
      "managed"
    ],
    [
      "%",
      "of",
      "on"
    ],
    [
      "variety",
      "of",
      "on"
    ],
    [
      "series",
      "of",
      "as"
    ],
    [
      "tokens",
      "in",
      "like"
    ],
    [
      "pattern",
      "for",
      "follow"
    ],
    [
      "series",
      "of",
      "into"
    ],
    [
      "representation",
      "of",
      "by"
    ],
    [
      "autoencoder",
      "to",
      "by"
    ],
    [
      "representation",
      "of",
      "generates"
    ],
    [
      "success",
      "in",
      "had"
    ],
    [
      "models",
      "as",
      "demonstrate"
    ],
    [
      "ability",
      "of",
      "demonstrate"
    ],
    [
      "variety",
      "of",
      "perform"
    ],
    [
      "variety",
      "including",
      "perform"
    ],
    [
      "success",
      "in",
      "had"
    ],
    [
      "applications",
      "as",
      "in"
    ],
    [
      "Elo",
      "of",
      "achieved"
    ]
  ],
  "stats": {
    "total_entities": 210,
    "unique_entities": 198,
    "total_relationships": 263
  }
}